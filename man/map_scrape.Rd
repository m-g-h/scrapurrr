% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/crawler.R
\name{map_scrape}
\alias{map_scrape}
\title{Apply a function to each element in a list or atomic vector. Specialised for
scrapurrr}
\usage{
map_scrape(
  ...,
  .f,
  timeout = 15,
  attempts = 3,
  print_status_message = T,
  parallel_workers = 0,
  parallel_strategy = NULL
)
}
\arguments{
\item{...}{Additional arguments passed on to the mapped function.}

\item{.f}{\code{function}. A function that returns a named list.}

\item{timeout}{\code{numeric scalar}. Seconds until function is considered to
be timed out.}

\item{attempts}{\code{numeric scalar}. Number of attempts to retry execution
of \code{.f}.}

\item{print_status_message}{\code{Logical scalar}, indicating whether status
messages should be printed.}

\item{parallel_workers}{\code{numeric scalar} giving the number of parallel
workers for parallelisation}

\item{parallel_strategy}{\code{string scalar} giving the strategy for parallelisation.
Also see \code{\link[future]{plan}}}
}
\value{
Returns a \code{tibble} with the scraping results. Includes a column
with a numeric index \code{n}.
}
\description{
This function is a wrapper around \code{\link[purrr]{pmap_dfr}} and works
similarly: provide a list of links or html files you want to scrape and a
function containing the scraping logic. \code{map_scrape()} then applies
the function to each element in the list and returns a \code{tibble}.
}
\details{
This function automatically prints a status message for each object passed to
\code{.f} using \code{\link{messagefun}}, giving you status updates for long
jobs.

It also uses \code{\link{try_and_timeout}} internally to capture errors or
timeouts. Instead of stopping on an error it continues to scrape. In such
cases, the error message is returned as \code{character} in the output.

\strong{Parallelisation} is implemented via \code{\link[furrr]{future_pmap_dfr}}
To scrape in parallel, simply provide the arguments \code{parallel_workers}
and \code{parallel_strategy}
}
\examples{
library(magrittr)
library(rvest)
library(scrapurrr)

# List of pages to scrape
links = list("https://de.wikipedia.org/wiki/Bayern",
             "https://de.wikipedia.org/wiki/Berlin",
             "https://de.wikipedia.org/wiki/Brandenburg")

# Web scraping function
scrapefun = function(link){
  title = read_html(link) \%>\%
    html_elements("h1") \%>\%
    html_text()
 list("title" = title)
}
links \%>\%
  map_scrape(.f = scrapefun)
}
