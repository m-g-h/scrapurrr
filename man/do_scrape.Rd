% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/crawler.R
\name{do_scrape}
\alias{do_scrape}
\title{Scrape content of objects using a specified function. Supports timeouts and
error handling.}
\usage{
do_scrape(.func, ..., timeout = 15, attempts = 3, print_status_message = T)
}
\arguments{
\item{.func}{\code{function} A function that returns a list with named
entries}

\item{...}{arguments to the function, e.g. a list of links to scrape from}

\item{timeout}{\code{numeric scalar} Seconds until function is considered to
be timed out}

\item{attempts}{\code{numeric scalar} Number of attempts to try the function}

\item{print_status_message}{\code{Logical scalar} indicating whether status
messages should be printed}
}
\value{
Returns a \code{tibble}
}
\description{
Scrape content of objects using a specified function. Supports timeouts and
error handling.
}
\examples{

library(magrittr)
library(rvest)
library(webscraping)

# List of pages to scrape
links = list("https://de.wikipedia.org/wiki/Bayern",
             "https://de.wikipedia.org/wiki/Berlin",
             "https://de.wikipedia.org/wiki/Brandenburg")

# Web scraping function
scrapefun = function(link){
  title = read_html(link) \%>\%
    html_elements("h1") \%>\%
    html_text()
 list("title" = title)
}

do_scrape(scrapefun,links)
}
