---
title: "How To"
subtitle: "Introduction to the workflow and functionality of `scrapurrr`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{How To}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(scrapurrr)
```
This package is designed to provide tools for automating repetitive and/or challenging tasks of scrapurrr. Implementing the scrapurrr functionality itself is left to the user. In order to provide some guidance how to do that, this section introduces some basic concepts using the package `rvest` ([website](https://rvest.tidyverse.org/)) and the popular `tidyverse` as data wrangling backend.

**Contents:**

- Manual Workflow: Scraping a Single Page
- Automated Workflow 1: Scraping Multiple Pages with `map_scrape()`
- Functionality of `map_scrape()`
- Automated Workflow 2: On-Line Download, Off-Line Extraction (for challenging projects)
- Overcoming Webscraping Obstacles
- Dealing with request limits using timing and proxies
- Downloading dynamically generated content / `javascript` rendered pages
- Extracting `javascript` obfuscated data in `R` with `V8`

## Scraping a Single Page

Suppose we need to know the capital city of Bavaria. This info is easily available, e.g. on [wikipedia](https://en.wikipedia.org/wiki/Bavaria). Using `rvest` we can download the page `html` and extract the information:

```{r message=FALSE, warning=FALSE}

library(rvest)
library(tidyverse)

# Download the html of "https://en.wikipedia.org/wiki/Bavaria"
page = read_html("https://en.wikipedia.org/wiki/Bavaria")

# Extract name of country
page %>% 
  html_element("hcapture the download1") %>% 
  html_text()

# Extract capital city
page %>% 
  html_element("table.infobox") %>% 
  html_table(header = F) %>% 
  filter(str_detect(X1, "Capital")) %>% 
  pull("X2")

```

## Automated Workflow 1: Scraping Multiple Pages with `map_scrape()`

Usually one needs to scrape multiple pages. E.g. we could be interested in the capital cities of *each* of the 16 German states. The simple scrapurrr process implemented in this package is descrived in the following sections.

### 1. Define the list of links from which data needs to be extracted

```{r}
# We could search the wikipedia links manually - or use scrapurrr:
state_links = read_html("https://en.wikipedia.org/wiki/States_of_Germany") %>% 
  html_element(".wikitable") %>% 
  html_elements("tr> td:nth-child(3)") %>% 
  html_elements("a") %>% 
  html_attr("href") %>% 
  paste0("https://en.wikipedia.org", .)

glimpse(state_links)

```

### 3. Define an **extraction function** capturing the repetitive scrapurrr tasks :

The following function captures the complete scrapurrr task: downloading the `html` and extracting the relevant content. For use in the `map_scrape()` function provided in this package one thing is important: that it returns a *named list*. This allows to present the output as a dataframe.

Sometimes there are more challenges, like dynamically created content that can't be downloaded via `rvest` or obfuscated data (e.g. e-mails). Ways to deal with these challenges are described in the section **Overcoming Webscraping Obstacles**

```{r}
# Define the extraction function
get_capitals = function(link){
  # Download html
  .page = read_html(link)
  
  # Extract name of country
  State = .page %>% 
    html_element("h1") %>% 
    html_text()
  
  # Extract capital city
  Capital = .page %>% 
    html_element("table.infobox") %>% 
    html_table(header = F) %>% 
    filter(str_detect(X1, "Capital")) %>% 
    pull("X2")
  
  # Return results as named list
  return_named_list()
}

```

### 3. Apply the function to each link in the list
For this purpose, `webscraper` offers the `map_scrape()` function. It is basically a wrapper around `pmap_dfr()` from `purrr` that provides some scrapurrr-specific utilities. One is that it prints informative status messages to the console, which are useful for long scraping tasks (for further info see the section **Functionality of `map_scrape()`**. Let's use it for our search of capitals:
```{r message=TRUE, warning=TRUE}
library(scrapurrr)
# Map the function to all state links and merge the results by row into a dataframe
results = map_scrape(.f = get_capitals,
                    state_links)

# Print results
glimpse(results)
```

## Functionality of `map_scrape()`

`map_scrape()` is the central function of the scrapurrr workflow in this package. It is basically a wrapper around `pmap_dfr()` from `purrr` and behaves just like it. It also solves some scrapurrr specific tasks:

**Status Messages**
In really large scraping projects it can be useful to see how many objects have been processed, what object is currently being processed and how many there are in total. If the argument `print_status_message` is set to TRUE (default) such a message is printed for every object.

**Error and Timeout handling**
When scraping a large amount of pages, it can be frustrating to have an error after 70% of the process is completed. This can either happen because of a bug in the extraction function, or if there is an issue with your or the pages internet connection. In these cases, `map_scrape()` does not simply break, but retries to obtain the results. You can specify the amount of retries using the `attempts` argument. For problems with the internet connecion, instead of waiting an immense amount of time, `map_scrape()` fails after the amount of seconds specified using the `timeout` argument.

If an error or timeout occurs, the output contains the error message and the position at which the error occured in the output.

**Limit Request Frequency**
sys.sleep

**Proxy Rotation**
experimental

## Useful Helper Functions

### Easier Scraping Function Output via `return_named_list()`

The extraction functions used in `map_scrape()` are supposed to return a named list. This allows to bind together multiple results row wise. Usually, this is implemented as:

```{r}

# Without `return_named_list()`
get_capitals = function(link){
  # Download html
  page = read_html(link)
  
  # Extract name of country
  State = page %>% 
    html_element("h1") %>% 
    html_text()
  
  # Extract capital city
  Capital = page %>% 
    html_element("table.infobox") %>% 
    html_table(header = F) %>% 
    filter(str_detect(X1, "Capital")) %>% 
    pull("X2")
  
  # Return results as named list
  list("State" = State,
       "Capital" = Capital)
}

get_capitals("https://en.wikipedia.org/wiki/Bavaria")

```

This can become tedious if there are many outputs. `return_named_list()` takes up and automates this tedious task. Just prefix all objects that should not be returned with a dot (e.g. `.page`):

```{r}

# With `return_named_list()`
get_capitals = function(link){
  # Download html
  .page = read_html(link)
  
  # Extract name of country
  State = .page %>% 
    html_element("h1") %>% 
    html_text()
  
  # Extract capital city
  Capital = .page %>% 
    html_element("table.infobox") %>% 
    html_table(header = F) %>% 
    filter(str_detect(X1, "Capital")) %>% 
    pull("X2")
  
  # Return results as named list
  return_named_list()
}

get_capitals("https://en.wikipedia.org/wiki/Bavaria")
```

### Extraction using Regex via `node_which()` and `html_find()`

In the extraction step of scrapurrr one usually selects `nodes` using `css` or `XPath`, e.g. using the `rvest` packages `html_elements()`. However, sometimes these selectors are not enough. Consider the following `html`:

```{html}
<table>
  <tr>
    <th>Company</th>
    <th>Contact</th>
    <th>Country</th>
  </tr>
  <tr>
    <td>Alfreds Futterkiste</td>
    <td>Maria Anders</td>
    <td>Germany</td>
  </tr>
  <tr>
    <td>Centro comercial Moctezuma</td>
    <td>Francisco Chang</td>
    <td>Mexico</td>
  </tr>
</table>
```

This table has the three columns "Company", "Contact" and "Country". Suppose we want to get the contact to "Alfreds Futterkiste". Simply selecting all `td` nodes and then matching positions might work:
```{r include=FALSE}

html = "<table>
  <tr>
    <th>Company</th>
    <th>Contact</th>
    <th>Country</th>
  </tr>
  <tr>
    <td>Alfreds Futterkiste</td>
    <td>Maria Anders</td>
    <td>Germany</td>
  </tr>
  <tr>
    <td>Centro comercial Moctezuma</td>
    <td>Francisco Chang</td>
    <td>Mexico</td>
  </tr>
</table>" %>% 
  xml2::read_html()

```

```{r}
# I parsed the html into the object `html` in the background
library(rvest)

# Manual selection works
html %>% 
  html_elements("td") %>% 
  .[2]

# CSS selection by child position works:
html %>% 
  html_elements("tr:nth-child(2) > td:nth_child(2)")
```

But what if you have multiple tables and the position of the information (i.e. the row number) changes from table to table? This can be solved by `regex` selection of the text as implemented by `node_which()` and `html_find()`:

```{r}
library(scrapurrr)
# Get the position of the "Contact" to "Alfreds Futterkiste"
html %>% 
  html_elements("td") %>% 
  node_which("Alfreds Futterkiste", inc = 1)

# Get the node of the "Contact" to "Alfreds Futterkiste"
html %>% 
  html_elements("td") %>% 
  html_find("Alfreds Futterkiste", inc = 1)

```
Note that we use `inc = 1` in order to not select the `td` that contains "Alfreds Futterkiste", but the node that comes afterwards. In the context of a table this corresponds to the next column.

### Display Downloaded `html` and Pre-Testing via `view_html()`

The convenience funtion `view_html()` allows to view what the computer sees. Either provide a link to a wepage or the `html` directly. 

This is useful for **pretesting**, i.e. for checking what the computer gets (and what it doesn't get). By using `view_html()` you can quickly judge if you need a webdriver like `Selenium` or `PhantomJS` or if simple scraping methods, i.e, `rvest`are sufficient.

### Shuffle NordVPN Servers using `NordVPN_quick_connect()`

If you have NordVPN installed, you can invoke the `Quick Connect` command from R via `system('"Path\\to\\NordVPN.exe" -c')`. This is what `NordVPN_quick_connect()` does if you invoke it. It also checks if a connection was established by trying to `GET` www.google.com and automatically tries a new proxy after 20 seconds.


## Automated Workflow 2: On-Line Download, Off-Line Extraction (for challenging projects)
Sometimes it is neccessary to separate the download of `html` from the extraction. This can be the case when there is a request limit on the desired page and one can not endlessly trial-and-error until the extraction function works, or when the page is rendered dynamically. A third reason is reproducibility and documentation of the raw `html`. In these cases it makes sense to first download all pages into `html` files on the harddrive and then extract them in a second step.

This process can also be implemented using `map_scrape`:

### On-Line: Downloading the html

```{r}

# We reuse the list of wikipedia pages `state_list` from before
glimpse(state_links)

# For this example we use a temporary folder to store the html files
tmp_folder = tempdir()

# Define the downloading function. We index each page with an id
download_html = function(link, id){
  
  # Define the filename
  file = file.path(tmp_folder,
                   id) %>% 
    paste0(., ".html")
  
  # Write the file to harddrive
  read_html(link) %>% 
    as.character() %>% 
    write_lines(file = file)
  
  # Return file path
  list("file" = file)
}

# Define id
id = 1:length(state_links)

# Scrape and save html
results = map_scrape(state_links, .f = download_html, id)

# The results in the dataframe
glimpse(results)

# The files on the harddrive
glimpse(list.files(tmp_folder,
                   pattern = "html"))

```

### Off-Line: Extracting the Information
Now that we have the `html` as files on the hard drive, we have all the time in the world to experiment with our extraction function without the need to re-download the `html` each time.

We can actually use the exaxt same function as in the section above **Automated Workflow 1: Scraping Multiple Pages with `map_scrape()`**. The `link` argument now needs to lead to the file on the harddrive.

```{r}
# Define the extraction function
get_capitals = function(link){
  # Download html
  .page = read_html(link)
  
  # Extract name of country
  State = .page %>% 
    html_element("h1") %>% 
    html_text()
  
  # Extract capital city
  Capital = .page %>% 
    html_element("table.infobox") %>% 
    html_table(header = F) %>% 
    filter(str_detect(X1, "Capital")) %>% 
    pull("X2")
  
  # Return results as named list
  return_named_list()
}

# The files on the harddrive
files = list.files(tmp_folder,
                   full.names = T,
                   pattern = "html")

# Extraction
results = map_scrape(files,
                     .f = get_capitals)

# The results in the dataframe
glimpse(results)

```



